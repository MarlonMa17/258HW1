{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0585811",
   "metadata": {},
   "source": [
    "# Install ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a13fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.17.0)\n",
      "Collecting onnxscript\n",
      "  Downloading onnxscript-0.2.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.24.4)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.12.2)\n",
      "Collecting ml_dtypes (from onnxscript)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (23.2)\n",
      "Downloading onnxscript-0.2.3-py3-none-any.whl (700 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.2/700.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ml_dtypes, onnxscript\n",
      "Successfully installed ml_dtypes-0.5.1 onnxscript-0.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting onnxruntime\n",
      "  Downloading onnxruntime-1.21.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (4.24.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnxruntime-1.21.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Installing collected packages: flatbuffers, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-25.2.10 humanfriendly-10.0 onnxruntime-1.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting onnxruntime-gpu\n",
      "  Downloading onnxruntime_gpu-1.21.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (23.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (4.24.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Downloading onnxruntime_gpu-1.21.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (280.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.8/280.8 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnxruntime-gpu\n",
      "Successfully installed onnxruntime-gpu-1.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install core ONNX export requirements\n",
    "!pip install --upgrade onnx onnxscript\n",
    "\n",
    "# Install ONNX Runtime for inference\n",
    "!pip install onnxruntime\n",
    "\n",
    "# If you have a GPU and want acceleration\n",
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b13c5f",
   "metadata": {},
   "source": [
    "# Convert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e2cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from timm import create_model\n",
    "\n",
    "def convert_densenet_to_onnx(model_path, output_path):\n",
    "    # Create a DenseNet121 model with 1 input channel instead of 3\n",
    "    model = models.densenet121()\n",
    "    \n",
    "    # Modify the first convolution layer to accept 1 channel instead of 3\n",
    "    # Save the original weights\n",
    "    original_conv = model.features.conv0\n",
    "    \n",
    "    # Create a new conv layer with 1 input channel but same output channels\n",
    "    model.features.conv0 = torch.nn.Conv2d(\n",
    "        1, \n",
    "        original_conv.out_channels,\n",
    "        kernel_size=original_conv.kernel_size,\n",
    "        stride=original_conv.stride,\n",
    "        padding=original_conv.padding,\n",
    "        bias=(original_conv.bias is not None)\n",
    "    )\n",
    "    \n",
    "    # Load the weights from the .pth file\n",
    "    state_dict = torch.load(model_path)\n",
    "    \n",
    "    # Handle different model save formats\n",
    "    if isinstance(state_dict, dict) and \"state_dict\" in state_dict:\n",
    "        state_dict = state_dict[\"state_dict\"]\n",
    "    \n",
    "    # Load the state dict with strict=False to allow for mismatches\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input with 1 channel instead of 3\n",
    "    dummy_input = torch.randn(1, 1, 224, 224)\n",
    "    \n",
    "    # Export the model to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=10,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    print(f\"Model has been converted to ONNX and saved at: {output_path}\")\n",
    "\n",
    "def convert_resnet50d_to_onnx(model_path, output_path):\n",
    "    # Load the state dict to check its structure\n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Check if the state dict has a nested structure\n",
    "    if isinstance(state_dict, dict) and \"state_dict\" in state_dict:\n",
    "        state_dict = state_dict[\"state_dict\"]\n",
    "    \n",
    "    # Option 1: Create model with correct number of classes (6)\n",
    "    model = create_model('resnet50d', pretrained=False, num_classes=6)\n",
    "    \n",
    "    # Option 2: Alternatively, remove the mismatched layers before loading\n",
    "    # model = create_model('resnet50d', pretrained=False)\n",
    "    # if 'fc.weight' in state_dict:\n",
    "    #     state_dict.pop('fc.weight')\n",
    "    # if 'fc.bias' in state_dict:\n",
    "    #     state_dict.pop('fc.bias')\n",
    "    \n",
    "    # Load state dict\n",
    "    try:\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading state dictionary: {e}\")\n",
    "        print(\"Attempting to modify state dict to match model structure...\")\n",
    "        \n",
    "        # Create a new filtered state dict\n",
    "        filtered_state_dict = {k: v for k, v in state_dict.items() \n",
    "                              if k not in ['fc.weight', 'fc.bias']}\n",
    "        model.load_state_dict(filtered_state_dict, strict=False)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=12,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    print(f\"Model has been converted to ONNX and saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6936a088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been converted to ONNX and saved at: ./model/monoai.onnx\n",
      "Model has been converted to ONNX and saved at: ./model/timm_base.onnx\n",
      "Model has been converted to ONNX and saved at: ./model/timm_optim.onnx\n"
     ]
    }
   ],
   "source": [
    "convert_resnet50d_to_onnx(\"./model/monoai.pth\",    \t\"./model/monoai.onnx\")\n",
    "convert_resnet50d_to_onnx(\"./model/timm_base.pth\", \t\"./model/timm_base.onnx\")\n",
    "convert_resnet50d_to_onnx(\"./model/timm_optim.pth\", \"./model/timm_optim.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc0cde",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae85499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the preprocessing transforms\n",
    "# These are standard transforms for ResNet models\n",
    "transform = transforms.Compose([\n",
    "\ttransforms.Resize(256),\n",
    "\ttransforms.CenterCrop(224),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Path to test folder\n",
    "test_dir = \"./data/test\"\n",
    "\n",
    "# Create dataset from test folder\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Create data loader\n",
    "test_loader = DataLoader(\n",
    "\ttest_dataset,\n",
    "\tbatch_size=32,\n",
    "\tshuffle=False,\n",
    "\tnum_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Test dataset contains {len(test_dataset)} images\")\n",
    "print(f\"Class mapping: {test_dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_onnx_model(model_path, test_loader, device='cpu'):\n",
    "    print(f\"\\nRunning model: {model_path} on {device}\")\n",
    "    \n",
    "    # Set up ONNX Runtime session based on device\n",
    "    if device == 'cpu':\n",
    "        session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "    else:\n",
    "        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        session = ort.InferenceSession(model_path, providers=providers)\n",
    "    \n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "    \n",
    "    # Collect predictions and ground truth\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_time = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Run inference on test data\n",
    "    for inputs, labels in test_loader:\n",
    "        batch_size = inputs.shape[0]\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        # Convert to numpy for ONNX Runtime\n",
    "        inputs_np = inputs.numpy()\n",
    "        \n",
    "        # Time the inference\n",
    "        start_time = time.time()\n",
    "        outputs = session.run([output_name], {input_name: inputs_np})[0]\n",
    "        end_time = time.time()\n",
    "        \n",
    "        inference_time = end_time - start_time\n",
    "        total_time += inference_time\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = np.argmax(outputs, axis=1)\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate accuracy and other metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_time_per_batch = total_time / len(test_loader)\n",
    "    avg_time_per_sample = total_time / total_samples\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Total inference time: {total_time:.4f} seconds\")\n",
    "    print(f\"Average time per batch: {avg_time_per_batch:.4f} seconds\")\n",
    "    print(f\"Average time per sample: {avg_time_per_sample*1000:.4f} ms\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=list(test_dataset.class_to_idx.keys())))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'total_time': total_time,\n",
    "        'avg_time_per_batch': avg_time_per_batch,\n",
    "        'avg_time_per_sample': avg_time_per_sample\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to evaluate\n",
    "models = [\n",
    "    \"./model/monoai.onnx\",\n",
    "    \"./model/timm_base.onnx\",\n",
    "    \"./model/timm_optim.onnx\"\n",
    "]\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = 'CUDAExecutionProvider' in ort.get_available_providers()\n",
    "print(f\"Available ONNX Runtime providers: {ort.get_available_providers()}\")\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# Run all models on CPU\n",
    "cpu_results = {}\n",
    "for model_path in models:\n",
    "    model_name = os.path.basename(model_path)\n",
    "    cpu_results[model_name] = run_onnx_model(model_path, test_loader, device='cpu')\n",
    "\n",
    "# Run all models on GPU if available\n",
    "gpu_results = {}\n",
    "if cuda_available:\n",
    "    for model_path in models:\n",
    "        model_name = os.path.basename(model_path)\n",
    "        gpu_results[model_name] = run_onnx_model(model_path, test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c89f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "def plot_comparison(cpu_results, gpu_results=None):\n",
    "    models = list(cpu_results.keys())\n",
    "    metrics = ['accuracy', 'avg_time_per_sample']\n",
    "    titles = ['Accuracy', 'Average Inference Time per Sample (ms)']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        cpu_values = [cpu_results[model][metric] for model in models]\n",
    "        \n",
    "        # Convert time to milliseconds for better readability\n",
    "        if metric == 'avg_time_per_sample':\n",
    "            cpu_values = [v * 1000 for v in cpu_values]  # Convert to ms\n",
    "        \n",
    "        x = np.arange(len(models))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[i].bar(x - width/2 if gpu_results else x, cpu_values, width, label='CPU')\n",
    "        \n",
    "        if gpu_results:\n",
    "            gpu_values = [gpu_results[model][metric] for model in models]\n",
    "            if metric == 'avg_time_per_sample':\n",
    "                gpu_values = [v * 1000 for v in gpu_values]  # Convert to ms\n",
    "            axes[i].bar(x + width/2, gpu_values, width, label='GPU')\n",
    "        \n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels([m.split('.')[0] for m in models], rotation=45)\n",
    "        \n",
    "        if metric == 'accuracy':\n",
    "            axes[i].set_ylim(0, 1.1)\n",
    "        \n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparison\n",
    "if gpu_results:\n",
    "    plot_comparison(cpu_results, gpu_results)\n",
    "else:\n",
    "    plot_comparison(cpu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table comparing all results\n",
    "import pandas as pd\n",
    "\n",
    "def create_comparison_table(cpu_results, gpu_results=None):\n",
    "    data = []\n",
    "    for model_name in cpu_results.keys():\n",
    "        model_base = model_name.split('.')[0]\n",
    "        row = {\n",
    "            'Model': model_base,\n",
    "            'CPU Accuracy': f\"{cpu_results[model_name]['accuracy']:.4f}\",\n",
    "            'CPU Inference Time (ms)': f\"{cpu_results[model_name]['avg_time_per_sample']*1000:.2f}\"\n",
    "        }\n",
    "        \n",
    "        if gpu_results:\n",
    "            row.update({\n",
    "                'GPU Accuracy': f\"{gpu_results[model_name]['accuracy']:.4f}\",\n",
    "                'GPU Inference Time (ms)': f\"{gpu_results[model_name]['avg_time_per_sample']*1000:.2f}\",\n",
    "                'Speed-up': f\"{cpu_results[model_name]['avg_time_per_sample']/gpu_results[model_name]['avg_time_per_sample']:.2f}x\"\n",
    "            })\n",
    "            \n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "comparison_table = create_comparison_table(cpu_results, gpu_results if 'gpu_results' in locals() else None)\n",
    "comparison_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
